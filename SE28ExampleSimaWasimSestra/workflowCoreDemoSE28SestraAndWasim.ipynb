{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "\n",
    "## How to use this notebook\n",
    "This example runs Sima in locally based on input created in the [Create load case folders with unique Sima input](#loadcase_id) section. The code is only a pilot and only intended for testing. \n",
    "\n",
    "This note bookshould be run in the following way:\n",
    "1. [Installation](#installation) can be run to install all the relevant tools, like python modules and local worker executable\n",
    "2. [Initialize Workflow](#initialize), run this Python code once at every notebook start to set up basic settings. [Set up custom user parameters](#custom) section should be changed if you want to change workspace or switch between cloud and local run. 2. If you changed something in this section, remember to rerun  [Set up OneWorkflow client](#builder).\n",
    "3. [Run analysis](#run) shall be run each time a new Sima analysis needs to be run.\n",
    "4. [Run Wasim and Sestra](#runwasim) run Wasim and Sestra using results from above Sima run.\n",
    "\n",
    "### Postprocessing\n",
    "It is now possible to read the SIN/FEM file using SifIO, please consult the [Python examples](https://test.pypi.org/project/dnv-sifio/) C# [documentation](https://sesam.dnv.com/dev/api/sifio/). This notebook provides a small example script for getting node coordinates and displacements for the 200 first nodes for loadcase 11, [postprocessing.py](Workspace/CommonFiles/postprocessing.py). The script will be run just after Wasim and Sestra for each load case, see the [Run Wasim and Sestra](#runwasim) section. The section [Post processing](#postprocessing) shows how the post processed result files may be read and visualized in this notebook. More Python examples will be provided later. Relevant additional documents are:\n",
    "- The input interface file documentations can be found [here](https://sesam.dnv.com/download/windows64/sesam_input_interface_format.pdf)\n",
    "- The results interface format [here](https://sesam.dnv.com/download/windows64/sesam_results_interface_format.pdf)\n",
    "\n",
    "The Python package should be installed with this command : pip install -i https://test.pypi.org/simple/ dnv-sifio --user as done in the [Installation](#installation) section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize workflow <a id='initialize'></a>\n",
    "Run only once when notebook is opened."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation  <a id='installation'></a>\n",
    "Run only when packages and/or local worker  needs to be upgraded. Set _update_packages_ to True first time and when packages should be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pythonnet\n",
      "  Using cached pythonnet-3.0.1-py3-none-any.whl (284 kB)\n",
      "Requirement already satisfied: clr-loader<0.3.0,>=0.2.2 in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pythonnet) (0.2.5)\n",
      "Requirement already satisfied: cffi>=1.13 in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clr-loader<0.3.0,>=0.2.2->pythonnet) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.13->clr-loader<0.3.0,>=0.2.2->pythonnet) (2.21)\n",
      "Installing collected packages: pythonnet\n",
      "Successfully installed pythonnet-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting azure-storage-blob\n",
      "  Obtaining dependency information for azure-storage-blob from https://files.pythonhosted.org/packages/f1/06/68c50a905e1e5481b04a6166b69fecddb87681aae7a556ab727f8e8e6f70/azure_storage_blob-12.17.0-py3-none-any.whl.metadata\n",
      "  Using cached azure_storage_blob-12.17.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-storage-blob) (1.29.2)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-storage-blob) (38.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-storage-blob) (4.6.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-storage-blob) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kblu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2023.5.7)\n",
      "Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB)\n",
      "Installing collected packages: azure-storage-blob\n",
      "Successfully installed azure-storage-blob-12.17.0\n"
     ]
    }
   ],
   "source": [
    "update_packages = True\n",
    "version = \"0.1.0.563\"\n",
    "version_runtime = \"0.1.0.550\"\n",
    "if update_packages:\n",
    "    #Uninstall all OneWorkflow packages\n",
    "    %pip uninstall -y dnv-runtime dnv-dotnet-intellisense dnv-sifio dnv-sesam-commands dnv-oneworkflow dnv-onecompute \n",
    "\n",
    "    #Uninstall additional packages\n",
    "    %pip uninstall -y pythonnet quantconnect-stubs azure-storage-blob\n",
    "    %pip install azure-storage-blob\n",
    "    #(User Site, with '--user' flag)\n",
    "    %pip install pythonnet --user\n",
    "    %pip install quantconnect-stubs --user\n",
    "    %pip install -i https://test.pypi.org/simple/ dnv-runtime==$version_runtime --user\n",
    "    %pip install -i https://test.pypi.org/simple/ dnv-dotnet-intellisense==$version --user & update-stubs\n",
    "    %pip install -i https://test.pypi.org/simple/ dnv-sifio==$version --user\n",
    "\n",
    "    #Others(Non User Site, without '--user' flag)\n",
    "    %pip install asyncio \n",
    "    %pip install -i https://test.pypi.org/simple/ dnv-onecompute==$version\n",
    "    %pip install -i https://test.pypi.org/simple/ dnv-oneworkflow==$version\n",
    "    %pip install -i https://test.pypi.org/simple/ dnv-sesam-commands==$version\n",
    "    %pip uninstall -y asyncio\n",
    "    from oneWorkflowToolBox import *\n",
    "    await install_workflow_runtime()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: The installed package 'LocalWorkflowRuntime' (version 0.1.0.563) is the latest\n"
     ]
    }
   ],
   "source": [
    "from oneWorkflowToolBox import *\n",
    "await install_workflow_runtime()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up custom user parameters <a id='custom'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# local workspace, all results will be put here after local or cloud runs\n",
    "workspacePath = r'C:\\Users\\kblu\\source\\repos\\improveflowH4-workflow\\SE28ExampleSimaWasimSestra\\workspace'\n",
    "# location of common files for all analysis, has to be below workspacePath\n",
    "workspaceId = \"SE28\"\n",
    "loadcase_file = f\"{workspacePath}\\\\test_cases.xlsx\"\n",
    "wasim_input_file = \"test_cases_wasim_input.xlsx\"\n",
    "stask_file = \"SimaTemplate.stask\"\n",
    "cloud_run = False\n",
    "notebook_root_folder = os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up OneWorkflow client <a id='builder'></a>\n",
    "Run only once workbook is started or if some parameters above are changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oneWorkflowToolBox import *\n",
    "workflow_client = one_workflow_client(\n",
    "    workspaceId, workspacePath, cloud_run, tmp=r\"c:\\OneWorkflowTemp\") \n",
    "workspace = workflow_client.one_workflow_config.workspace_config\n",
    "commonfiles_folder = workspace.common_files_directory\n",
    "results_folder = workspace.results_directory\n",
    "#If running locally the code below will also start the local workflow host.\n",
    "if (cloud_run):\n",
    "    workflow_client.login()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload common files for the job <a id='upload'></a>\n",
    "This step uploads all common files in folder *commonFilesDirectory*  to the job. Only needed to run if new common files are to be uploaded or workspace changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dnv.onecompute.directory_client import FileOptions\n",
    "try:\n",
    "    workflow_client.upload_common_files(FileOptions(\n",
    "        # max_size_bytes=124_000,\n",
    "        #patterns=[\"**/*.py\",\"**/*.inp\"],\n",
    "        overwrite=True))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Ignore this error message if the files are already present.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Sima analysis <a id='run'></a>\n",
    "This code will fetch data from the blob storage created in the step above, and run all the job tasks. The code will wait for all tasks to complete before downloading the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oneWorkflowToolBox import run_workflow_async\n",
    "from SimaHelper import *\n",
    "import json\n",
    "\"\"\"Tests SIMA and Python commands\"\"\"\n",
    "# Upload Input Files\n",
    "workflow_client.upload_input_files()\n",
    "\n",
    "#Sima path must be specified\n",
    "sima_settings = SimaSettings(sima_exe_path=r'C:\\Program Files\\DNV\\Sima V4.4-00')\n",
    "sima_work_unit = SimaTaskCreator(sima_settings, workflow_client).get_sima_work_unit(loadcase_file, stask_file)\n",
    "\n",
    "# Create Parallel Work Unit and Job\n",
    "job = workflow_client.create_job(sima_work_unit)\n",
    "job_json = json.dumps(job, default=lambda o: o.encode(), indent=4)\n",
    "\n",
    "#print(job_json)\n",
    "# Run workflow\n",
    "downloadOptions = FileOptions(\n",
    "    max_size_bytes=11124_000,\n",
    "    patterns=[\"**/*.txt\", \"**/*.lis\", \"**/*.mlg\"])\n",
    "await run_workflow_async(job, workflow_client, downloadOptions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Wasim and Sestra <a id='runwasim'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnv.oneworkflow import PythonCommand, CompositeExecutableCommand\n",
    "from dnv.sesam.wasim_command import WasimStruCommand,  WasimSetupCommand\n",
    "from dnv.onecompute.flowmodel import WorkUnit, SchedulingOptions, FailureStrategy\n",
    "from SesamUtilities import WasimAndSestraTaskCreator\n",
    "import pandas as pd\n",
    "from dnv.oneworkflow import  ParallelWork\n",
    "from oneWorkflowToolBox import Platform\n",
    "import json\n",
    "import os\n",
    "from dnv.sesam.sestra_command import SestraCommand\n",
    "workspace.results_directory = \"Results\"\n",
    "load_cases = [\"test001\", \"test002\"]\n",
    "\n",
    "os.chdir(workspacePath)\n",
    "topSuperElement = 3\n",
    "# due to 5 field width on Sestra cards we need to use as short name here\n",
    "additionalTemplateParameters = {\n",
    "    'FMT': topSuperElement, 'topsel': topSuperElement}\n",
    "\n",
    "def run_wasim_and_sestra_using_results_from_sima(\n",
    "        results_directory: str, load_cases: str) -> ParallelWork:\n",
    "    \"\"\"Creates a parallel work unit\"\"\"\n",
    "    df_cases = pd.read_excel(os.path.join(\n",
    "        workspacePath, wasim_input_file), index_col=0)\n",
    "    parallel_work_units = list[WorkUnit]()\n",
    "\n",
    "    for casename, case in df_cases.iterrows():\n",
    "        if not casename in load_cases:\n",
    "            print(\"skipping \" + casename)\n",
    "            continue\n",
    "        \n",
    "        load_case_result_files_dir = os.path.join(results_directory, casename)\n",
    "        casedict = case.to_dict()\n",
    "        \n",
    "        cmd = WasimAndSestraTaskCreator(\n",
    "            load_case_result_files_dir, commonfiles_folder, casedict, additionalTemplateParameters).CreateTasks()\n",
    "        work_unit = (\n",
    "            WorkUnit(cmd, f\"post_rerun_{casename}\")\n",
    "            .input_directory(load_case_result_files_dir)\n",
    "            .output_directory(load_case_result_files_dir, include_files=[\"**/sima.*\", \"**/*.txt\", \"**/*.tda\", \"**/*.bin\", \"**/*.log\", \"**/*.inp\", \"**/*.lis\", \"**/*.mlg\", \"**/*.sin\"])\n",
    "        )\n",
    "        parallel_work_units.append(work_unit)\n",
    "\n",
    "    return ParallelWork(parallel_work_units)\n",
    "print(os.getcwd())\n",
    "print(workflow_client.workspace_path)\n",
    "\n",
    "work_unit = run_wasim_and_sestra_using_results_from_sima(\n",
    "    workspace.results_directory, load_cases)\n",
    "if not cloud_run:\n",
    "    workflow_client.upload_common_files()\n",
    "    workflow_client.upload_result_files()\n",
    "print(workflow_client.temp_path)\n",
    "\n",
    "\n",
    "downloadOptions = FileOptions(\n",
    "    min_size_bytes =0,\n",
    "    max_size_bytes=11124_000,\n",
    "    patterns=[\"**/*.txt\", \"**/*.lis\", \"**/*.mlg\", \"**/*.sin\"])\n",
    "await run_workflow_async(workflow_client.create_job(work_unit), workflow_client, downloadOptions)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing  <a id='postprocessing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from ipywidgets import interactive\n",
    "\n",
    "lc = 11\n",
    "dataFrames = {}\n",
    "print(workspacePath)\n",
    "for folder in glob.glob(f\"{workspacePath}\\\\Results\\\\*\"):\n",
    "    test_name = folder.split(\"\\\\\")[-1]\n",
    "    try:\n",
    "        data = np.loadtxt(f\"{folder}\\\\postprocessedresultsLC{lc}.txt\")\n",
    "        dispdata = {\n",
    "            \"x-coord\": data[:, 0],\n",
    "            \"total-disp\":  np.sqrt(data[:, 3]**2+data[:, 4]**2+data[:, 5]**2)\n",
    "        }\n",
    "        dataFrames[test_name] = pd.DataFrame(dispdata)\n",
    "    except:\n",
    "        print(\"Failed loading data for test :\" + test_name)\n",
    "\n",
    "def multiplot(resultcase):\n",
    "    dataFrames[resultcase].plot(\n",
    "        x=\"x-coord\", y=\"total-disp\", kind=\"scatter\", figsize=(15, 15))\n",
    "\n",
    "\n",
    "interactive_plot = interactive(multiplot, resultcase=dataFrames.keys())\n",
    "interactive_plot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3f4950b6ecc2246e9f46f113f926a9a268224216b0af4259a912530ba1db262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
