{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This example is a simple demonstration of how to run a Sesam Core fatigue hotspot example using OneWorkflow locally and in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnv.oneworkflow.utils.workunit_extension import *\n",
    "from dnv.oneworkflow.utils.starter import *\n",
    "from pathlib import Path\n",
    "import os\n",
    "await install_workflow_runtime(repository = PackageManager.Repository.DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnv.oneworkflow.utils.workunit_extension import *\n",
    "from dnv.oneworkflow.utils.starter import *\n",
    "from dnv.oneworkflow import OneWorkflowClient\n",
    "from pathlib import Path\n",
    "import os\n",
    "oneWorkflowTMPFolder = r'c:\\OneWorkflowTMP' #due to possible issues with long file paths we prefer to have this folder at the root\n",
    "if not os.path.exists(oneWorkflowTMPFolder):\n",
    "    try:\n",
    "        print(\"Trying to create tmp folder for one workflow local execution\")\n",
    "        os.mkdir(oneWorkflowTMPFolder)\n",
    "    except:\n",
    "        print(\"did not manage to create tmp folder for local execution. Check that you have privileges to create it or try to manually create it from the coomand line.\")\n",
    "\n",
    "# local workspace, all results will be put here after local or cloud runs\n",
    "# location of common files for all analysis, has to be below workspacePath and in the folder names CommonFiles\n",
    "root_folder = os.getcwd()\n",
    "workspacePath = str(Path(root_folder, 'Workspace'))\n",
    "workspaceId = \"SesamCoreExample\"\n",
    "cloudRun = False\n",
    "#If running locally the code below will also start the local workflow host.\n",
    "workflow_client = one_workflow_client(workspace_id = workspaceId, workspace_path = workspacePath, cloud_run = cloudRun,\n",
    "                                      local_workflow_runtime_temp_folder_path = oneWorkflowTMPFolder, platform=Platform.WINDOWS, max_cores=5,auto_deploy_option = AutoDeployOption.DEV)\n",
    "workflow_client.start_workflow_runtime_service()\n",
    "if (cloudRun):\n",
    "    workflow_client.login()\n",
    "upload_success = workflow_client.upload_common_files(FileOptions(max_size=\"524MB\",patterns=[\"**/*.*\"], overwrite=True))\n",
    "\n",
    "# max number of threads used when running locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dnv.oneworkflow import  ParallelWork\n",
    "from dnv.onecompute.flowmodel import WorkUnit\n",
    "from dnv.sesam.sesam_core_command import *\n",
    "from dnv.oneworkflow import PythonCommand, CompositeExecutableCommand\n",
    "import json\n",
    "import shutil\n",
    "from dnv.oneworkflow.utils.workunit_extension import with_shared_files_copied_to_loadcase\n",
    "\n",
    "# we must delete existing results locally before generating new results\n",
    "local__result_path = Path(workspacePath, workflow_client.results_directory)\n",
    "if os.path.isdir(local__result_path):\n",
    "    shutil.rmtree(local__result_path) \n",
    "\n",
    "parallel_work = ParallelWork()\n",
    "for index in range(1,14): # iterating over two simple cases, they now will do the same analysis\n",
    "    loadcase_folder_name = f\"LoadCase{index}\"\n",
    "    result_folder_lc = os.path.join(workflow_client.results_directory, loadcase_folder_name)\n",
    "    python_copy_command = PythonCommand(\n",
    "        directory=workflow_client.common_directory)\n",
    "    core_command = SesamCoreCommand(command = \"fatigue\",input_file_name= \"Specimen1_input.json\", options = \"-hs\")\n",
    "    cmd = CompositeExecutableCommand([core_command], result_folder_lc)\n",
    "    work_unit = (WorkUnit(cmd, loadcase_folder_name)\n",
    "                 .output_directory(result_folder_lc, include_files=[\"**/*.txt\",  \"**/*.MLG\", \"**/*.lis\", \"**/*.sin\", \"*.log\"])\n",
    "                .with_shared_files_copied_to_loadcase(workflow_client.common_directory, [\"**/*.py\"])\n",
    "            )\n",
    "    parallel_work.WorkItems.append(work_unit)\n",
    "downloadOptions = FileOptions(max_size=\"10MB\",patterns=[\"**/*.txt\", \"**/*.lis\", \"**/*.MLG\"])\n",
    "job = workflow_client.create_job(parallel_work)\n",
    "#for debugging\n",
    "#job_json = json.dumps(job, default=lambda o: o.encode(), indent=4)\n",
    "#print(job_json)\n",
    "await run_workflow_async(job, workflow_client, downloadOptions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLose client -must be done before a new job can be started in a different notebook\n",
    "Will remove all job and blob folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_client.stop_workflow_runtime_service()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
